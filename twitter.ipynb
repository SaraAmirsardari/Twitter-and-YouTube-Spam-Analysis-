{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report, f1_score, precision_recall_curve\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,auc\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullFileName='twitter.csv'\n",
    "data=pd.read_csv(fullFileName, sep=' ')\n",
    "\n",
    "data_=data.replace(['1:','2:','3:','4:','5:','6:','7:','8:','9:','10:','11:','12:','13:','14:','15:','16:','17:','18:','19:',\n",
    "                   '20:','21:','22:','23:','24:','25:','25:','27:','28:','29:','30:','31:','32:','33:','34:','35:','36:','37:','38:','39:','40:','50:','60:'],'',regex=True).astype(float) \n",
    "\n",
    "data_.columns = ['user_class','Number of followers per followees','Fraction of tweets replied','Fraction of tweets with spam words',\n",
    "'Fraction of tweets with URLs','Existence of spam words in the screen name','Number of hashtags_mean',\n",
    "'Number of hashtags_median','Number of hashtags_min','Number of hashtage_max',\n",
    "'Number of URLs _mean','Number of URLs _median','Number of URLs _min','Number of URLs _max',\n",
    "'Number of characters per tweet_mean','Number of characters per tweet_median','Number of characters per tweet_min','Number of characters per tweet_max',\n",
    "'Number of hashtags per tweet_mean','Number of hashtags per tweet_median','Number of hashtags per tweet_min','Number of hashtags per tweet_max',\n",
    "'Number of mentions per tweet_mean','Number of mentions per tweet_median','Number of mentions per tweet_min','Number of mentions per tweet_max',\n",
    "'Number of numeric characters per tweet_mean','Number of numeric characters per tweet_median','Number of numeric characters per tweet_min','Number of numeric characters per tweet_max',\n",
    "'Number of URLs on each tweet_mean','Number of URLs on each tweet_median','Number of URLs on each tweet_min','Number of URLs on each tweet_max',\n",
    "'Number of words per tweet_mean','Number of words per tweet_median','Number of words per tweet_min','Number of words per tweet_max',\n",
    "'number of times the tweet has been retweeted_mean','number of times the tweet has been retweeted_median','number of times the tweet has been retweeted_min','number of times the tweet has been retweeted_max',\n",
    "'Number of followees','Number of followers','Number of tweets','Nnumber of followees of a user’s followers','Number of times mentioned',\n",
    "'Number of times the user was replied','Number of times the user replied','Number of tweets of a user’s followees','Time between posts_mean','Time between posts_median',\n",
    "'Time between posts_min','Time between posts_max','Number of posted tweets per day_mean','Number of posted tweets per day_median','Number of posted tweets per day_min',\n",
    "'Number of posted tweets per day_max','Number of posted tweets per day_mean','Number of posted tweets per day_median','Number of posted tweets per day_min','Number of posted tweets per day_max','Age of the user account']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght::  1064\n",
      "Dataset shape::  (1064, 63)\n"
     ]
    }
   ],
   "source": [
    "#first we count the number of missing values in each row in pandas dataframe\n",
    "data_.isnull().sum(axis=0)\n",
    "print(\"Dataset Lenght:: \",len(data_))\n",
    "print(\"Dataset shape:: \", data_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1064 entries, 0 to 1063\n",
      "Data columns (total 63 columns):\n",
      "user_class                                             1064 non-null float64\n",
      "Number of followers per followees                      1064 non-null float64\n",
      "Fraction of tweets replied                             1064 non-null float64\n",
      "Fraction of tweets with spam words                     1064 non-null float64\n",
      "Fraction of tweets with URLs                           1064 non-null float64\n",
      "Existence of spam words in the screen name             1064 non-null float64\n",
      "Number of hashtags_mean                                1064 non-null float64\n",
      "Number of hashtags_median                              1064 non-null float64\n",
      "Number of hashtags_min                                 1064 non-null float64\n",
      "Number of hashtage_max                                 1064 non-null float64\n",
      "Number of URLs _mean                                   1064 non-null float64\n",
      "Number of URLs _median                                 1064 non-null float64\n",
      "Number of URLs _min                                    1064 non-null float64\n",
      "Number of URLs _max                                    1064 non-null float64\n",
      "Number of characters per tweet_mean                    1064 non-null float64\n",
      "Number of characters per tweet_median                  1064 non-null float64\n",
      "Number of characters per tweet_min                     1064 non-null float64\n",
      "Number of characters per tweet_max                     1064 non-null float64\n",
      "Number of hashtags per tweet_mean                      1064 non-null float64\n",
      "Number of hashtags per tweet_median                    1064 non-null float64\n",
      "Number of hashtags per tweet_min                       1064 non-null float64\n",
      "Number of hashtags per tweet_max                       1064 non-null float64\n",
      "Number of mentions per tweet_mean                      1064 non-null float64\n",
      "Number of mentions per tweet_median                    1064 non-null float64\n",
      "Number of mentions per tweet_min                       1064 non-null float64\n",
      "Number of mentions per tweet_max                       1064 non-null float64\n",
      "Number of numeric characters per tweet_mean            1064 non-null float64\n",
      "Number of numeric characters per tweet_median          1064 non-null float64\n",
      "Number of numeric characters per tweet_min             1064 non-null float64\n",
      "Number of numeric characters per tweet_max             1064 non-null float64\n",
      "Number of URLs on each tweet_mean                      1064 non-null float64\n",
      "Number of URLs on each tweet_median                    1064 non-null float64\n",
      "Number of URLs on each tweet_min                       1064 non-null float64\n",
      "Number of URLs on each tweet_max                       1064 non-null float64\n",
      "Number of words per tweet_mean                         1064 non-null float64\n",
      "Number of words per tweet_median                       1064 non-null float64\n",
      "Number of words per tweet_min                          1064 non-null float64\n",
      "Number of words per tweet_max                          1064 non-null float64\n",
      "number of times the tweet has been retweeted_mean      1064 non-null float64\n",
      "number of times the tweet has been retweeted_median    1064 non-null float64\n",
      "number of times the tweet has been retweeted_min       1064 non-null float64\n",
      "number of times the tweet has been retweeted_max       1064 non-null float64\n",
      "Number of followees                                    1064 non-null float64\n",
      "Number of followers                                    1064 non-null float64\n",
      "Number of tweets                                       1064 non-null float64\n",
      "Nnumber of followees of a user’s followers             1064 non-null float64\n",
      "Number of times mentioned                              1064 non-null float64\n",
      "Number of times the user was replied                   1064 non-null float64\n",
      "Number of times the user replied                       1064 non-null float64\n",
      "Number of tweets of a user’s followees                 1064 non-null float64\n",
      "Time between posts_mean                                1064 non-null float64\n",
      "Time between posts_median                              1064 non-null float64\n",
      "Time between posts_min                                 1064 non-null float64\n",
      "Time between posts_max                                 1064 non-null float64\n",
      "Number of posted tweets per day_mean                   1064 non-null float64\n",
      "Number of posted tweets per day_median                 1064 non-null float64\n",
      "Number of posted tweets per day_min                    1064 non-null float64\n",
      "Number of posted tweets per day_max                    1064 non-null float64\n",
      "Number of posted tweets per day_mean                   1064 non-null float64\n",
      "Number of posted tweets per day_median                 1064 non-null float64\n",
      "Number of posted tweets per day_min                    1064 non-null float64\n",
      "Number of posted tweets per day_max                    1064 non-null float64\n",
      "Age of the user account                                1064 non-null float64\n",
      "dtypes: float64(63)\n",
      "memory usage: 523.8 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_class</th>\n",
       "      <th>Number of followers per followees</th>\n",
       "      <th>Fraction of tweets replied</th>\n",
       "      <th>Fraction of tweets with spam words</th>\n",
       "      <th>Fraction of tweets with URLs</th>\n",
       "      <th>Existence of spam words in the screen name</th>\n",
       "      <th>Number of hashtags_mean</th>\n",
       "      <th>Number of hashtags_median</th>\n",
       "      <th>Number of hashtags_min</th>\n",
       "      <th>Number of hashtage_max</th>\n",
       "      <th>...</th>\n",
       "      <th>Time between posts_max</th>\n",
       "      <th>Number of posted tweets per day_mean</th>\n",
       "      <th>Number of posted tweets per day_median</th>\n",
       "      <th>Number of posted tweets per day_min</th>\n",
       "      <th>Number of posted tweets per day_max</th>\n",
       "      <th>Number of posted tweets per day_mean</th>\n",
       "      <th>Number of posted tweets per day_median</th>\n",
       "      <th>Number of posted tweets per day_min</th>\n",
       "      <th>Number of posted tweets per day_max</th>\n",
       "      <th>Age of the user account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064000e+03</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>1.064000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.333647</td>\n",
       "      <td>1.275642</td>\n",
       "      <td>0.229468</td>\n",
       "      <td>0.134967</td>\n",
       "      <td>0.413622</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.030917</td>\n",
       "      <td>0.035823</td>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.403525</td>\n",
       "      <td>...</td>\n",
       "      <td>5.070877e+07</td>\n",
       "      <td>199.379297</td>\n",
       "      <td>201.963346</td>\n",
       "      <td>99.631579</td>\n",
       "      <td>969.672932</td>\n",
       "      <td>1017.616637</td>\n",
       "      <td>983.703947</td>\n",
       "      <td>11.556391</td>\n",
       "      <td>3338.031955</td>\n",
       "      <td>4.493992e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.471737</td>\n",
       "      <td>6.157445</td>\n",
       "      <td>0.212837</td>\n",
       "      <td>0.339470</td>\n",
       "      <td>0.357267</td>\n",
       "      <td>0.061227</td>\n",
       "      <td>0.046346</td>\n",
       "      <td>0.093575</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.309248</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131179e+08</td>\n",
       "      <td>477.707017</td>\n",
       "      <td>545.202549</td>\n",
       "      <td>366.903110</td>\n",
       "      <td>1573.957048</td>\n",
       "      <td>1709.653059</td>\n",
       "      <td>2284.329363</td>\n",
       "      <td>38.588460</td>\n",
       "      <td>7254.172127</td>\n",
       "      <td>2.596845e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>50.012900</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>50.090200</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>6.273771e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313268</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>...</td>\n",
       "      <td>5.281914e+06</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>55.997975</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>6.758591e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739994</td>\n",
       "      <td>0.191407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>...</td>\n",
       "      <td>5.823160e+06</td>\n",
       "      <td>53.093900</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>520.147300</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>657.000000</td>\n",
       "      <td>6.152921e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.738952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.256034e+07</td>\n",
       "      <td>59.053850</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>556.253800</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6146.000000</td>\n",
       "      <td>6.220431e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.020833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.606784e+08</td>\n",
       "      <td>5373.000000</td>\n",
       "      <td>5373.000000</td>\n",
       "      <td>5373.000000</td>\n",
       "      <td>5573.000000</td>\n",
       "      <td>5951.000000</td>\n",
       "      <td>51170.000000</td>\n",
       "      <td>732.000000</td>\n",
       "      <td>62870.000000</td>\n",
       "      <td>6.911490e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_class  Number of followers per followees  \\\n",
       "count  1064.000000                        1064.000000   \n",
       "mean      1.333647                           1.275642   \n",
       "std       0.471737                           6.157445   \n",
       "min       1.000000                           0.000000   \n",
       "25%       1.000000                           0.313268   \n",
       "50%       1.000000                           0.739994   \n",
       "75%       2.000000                           1.100000   \n",
       "max       2.000000                         166.020833   \n",
       "\n",
       "       Fraction of tweets replied  Fraction of tweets with spam words  \\\n",
       "count                 1064.000000                         1064.000000   \n",
       "mean                     0.229468                            0.134967   \n",
       "std                      0.212837                            0.339470   \n",
       "min                      0.000000                            0.000000   \n",
       "25%                      0.006253                            0.000000   \n",
       "50%                      0.191407                            0.000000   \n",
       "75%                      0.384394                            0.002291   \n",
       "max                      1.000000                            1.000000   \n",
       "\n",
       "       Fraction of tweets with URLs  \\\n",
       "count                   1064.000000   \n",
       "mean                       0.413622   \n",
       "std                        0.357267   \n",
       "min                        0.005457   \n",
       "25%                        0.112835   \n",
       "50%                        0.268197   \n",
       "75%                        0.738952   \n",
       "max                        1.000000   \n",
       "\n",
       "       Existence of spam words in the screen name  Number of hashtags_mean  \\\n",
       "count                                 1064.000000              1064.000000   \n",
       "mean                                     0.003759                 0.030917   \n",
       "std                                      0.061227                 0.046346   \n",
       "min                                      0.000000                 0.000000   \n",
       "25%                                      0.000000                 0.003700   \n",
       "50%                                      0.000000                 0.010700   \n",
       "75%                                      0.000000                 0.039750   \n",
       "max                                      1.000000                 0.388500   \n",
       "\n",
       "       Number of hashtags_median  Number of hashtags_min  \\\n",
       "count                1064.000000             1064.000000   \n",
       "mean                    0.035823                0.010807   \n",
       "std                     0.093575                0.041792   \n",
       "min                     0.000000                0.000000   \n",
       "25%                     0.000000                0.000000   \n",
       "50%                     0.000000                0.000000   \n",
       "75%                     0.000000                0.000000   \n",
       "max                     1.000000                0.375000   \n",
       "\n",
       "       Number of hashtage_max  ...  Time between posts_max  \\\n",
       "count             1064.000000  ...            1.064000e+03   \n",
       "mean                 0.403525  ...            5.070877e+07   \n",
       "std                  0.309248  ...            1.131179e+08   \n",
       "min                  0.000000  ...            5.000000e+01   \n",
       "25%                  0.166700  ...            5.281914e+06   \n",
       "50%                  0.272700  ...            5.823160e+06   \n",
       "75%                  0.500000  ...            5.256034e+07   \n",
       "max                  1.000000  ...            5.606784e+08   \n",
       "\n",
       "       Number of posted tweets per day_mean  \\\n",
       "count                           1064.000000   \n",
       "mean                             199.379297   \n",
       "std                              477.707017   \n",
       "min                               50.012900   \n",
       "25%                               51.000000   \n",
       "50%                               53.093900   \n",
       "75%                               59.053850   \n",
       "max                             5373.000000   \n",
       "\n",
       "       Number of posted tweets per day_median  \\\n",
       "count                             1064.000000   \n",
       "mean                               201.963346   \n",
       "std                                545.202549   \n",
       "min                                 50.000000   \n",
       "25%                                 50.000000   \n",
       "50%                                 51.000000   \n",
       "75%                                 58.000000   \n",
       "max                               5373.000000   \n",
       "\n",
       "       Number of posted tweets per day_min  \\\n",
       "count                          1064.000000   \n",
       "mean                             99.631579   \n",
       "std                             366.903110   \n",
       "min                              50.000000   \n",
       "25%                              50.000000   \n",
       "50%                              50.000000   \n",
       "75%                              50.000000   \n",
       "max                            5373.000000   \n",
       "\n",
       "       Number of posted tweets per day_max  \\\n",
       "count                          1064.000000   \n",
       "mean                            969.672932   \n",
       "std                            1573.957048   \n",
       "min                              51.000000   \n",
       "25%                              59.000000   \n",
       "50%                             525.000000   \n",
       "75%                             551.000000   \n",
       "max                            5573.000000   \n",
       "\n",
       "       Number of posted tweets per day_mean  \\\n",
       "count                           1064.000000   \n",
       "mean                            1017.616637   \n",
       "std                             1709.653059   \n",
       "min                               50.090200   \n",
       "25%                               55.997975   \n",
       "50%                              520.147300   \n",
       "75%                              556.253800   \n",
       "max                             5951.000000   \n",
       "\n",
       "       Number of posted tweets per day_median  \\\n",
       "count                             1064.000000   \n",
       "mean                               983.703947   \n",
       "std                               2284.329363   \n",
       "min                                 50.000000   \n",
       "25%                                 51.000000   \n",
       "50%                                513.000000   \n",
       "75%                                550.000000   \n",
       "max                              51170.000000   \n",
       "\n",
       "       Number of posted tweets per day_min  \\\n",
       "count                          1064.000000   \n",
       "mean                             11.556391   \n",
       "std                              38.588460   \n",
       "min                               0.000000   \n",
       "25%                               0.000000   \n",
       "50%                               1.000000   \n",
       "75%                               5.000000   \n",
       "max                             732.000000   \n",
       "\n",
       "       Number of posted tweets per day_max  Age of the user account  \n",
       "count                          1064.000000             1.064000e+03  \n",
       "mean                           3338.031955             4.493992e+08  \n",
       "std                            7254.172127             2.596845e+08  \n",
       "min                              61.000000             6.273771e+07  \n",
       "25%                             622.000000             6.758591e+07  \n",
       "50%                             657.000000             6.152921e+08  \n",
       "75%                            6146.000000             6.220431e+08  \n",
       "max                           62870.000000             6.911490e+08  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset:: \")\n",
    "data_.head()\n",
    "data_.info()\n",
    "data_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=data_.values\n",
    "X1 = array[:,1: ] \n",
    "Y = array[:,0]\n",
    " \n",
    "\"\"\" MIN MAX SCALER \"\"\"\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1)) \n",
    "# Scaled feature \n",
    "X2 = min_max_scaler.fit_transform(X1) \n",
    "  \n",
    "\"\"\" Standardisation \"\"\" \n",
    "Standardisation = preprocessing.StandardScaler() \n",
    "# Scaled feature \n",
    "X = Standardisation.fit_transform(X2) \n",
    "\n",
    "#Spliting Dataset into Test and Train\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2, random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate TP, TN,FN,FP\n",
    "def compute_tp_tn_fn_fp(actual_y, predicted_y):\n",
    "    #we define spam class(2) as positive rate, and not spam class(1) as negative rate\n",
    "    #True positive _ actual =2, predicted=2\n",
    "    #False positive _ actual=2, predicted=1\n",
    "    #False negative _ actual=1, predicted=2\n",
    "    #True negative _ actual=1, predicted=1\n",
    "    tp=sum((actual_y == 2) & (predicted_y == 2))\n",
    "    tn=sum((actual_y == 1) & (predicted_y == 1))\n",
    "    fn=sum((actual_y == 1) & (predicted_y == 2))\n",
    "    fp=sum((actual_y == 2) & (predicted_y == 1))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "#calculate accuracy\n",
    "def compute_accuracy(tp,tn,fn,fp):\n",
    "    #Acurracy=TP+TN/FP+FN+TP+TN\n",
    "    return((tp+tn*100)/float(tp+tn+fn+fp) if (tp+tn+fn+fp) != 0 else 0)\n",
    "\n",
    "#calculate precission\n",
    "def compute_precision(tp,fp):\n",
    "    #Acurracy=TP/FP+TP\n",
    "      return((tp*100)/float(tp+fp) if (tp+fp) != 0 else 0)\n",
    "    \n",
    "#calculate recall\n",
    "def compute_recall(tp,fn):\n",
    "     #recall=TP/FN+TP\n",
    "        return ((tp*100)/float(tp+fn) if (tp+fn) != 0 else 0)\n",
    "    \n",
    "#calculate f1_score\n",
    "def compute_f1_score(actual_y, predicted_y):\n",
    "        tp,tn,fp,fn= compute_tp_tn_fn_fp(actual_y, predicted_y)\n",
    "        precision=compute_precision(tp,fp)/100 \n",
    "        recall=compute_recall(tp,fn)/100\n",
    "        f1_score=(2*precision*recall)/(precision+recall) if (precision+recall) != 0 else 0\n",
    "        return f1_score\n",
    "\n",
    "def performance_of_the_model(actual_y,predicted_y): \n",
    "    \n",
    "    tp_r,tn_r, fp_r,fn_r=compute_tp_tn_fn_fp(actual_y,predicted_y)\n",
    "    print('TP for dt is:',tp_r)\n",
    "    print('TN for dt is:',tn_r)\n",
    "    print('FP for dt is:',fp_r)\n",
    "    print('FN for dt is:',fn_r)\n",
    "    accuracy=compute_accuracy(tp_r,tn_r,fn_r,fp_r)\n",
    "    print('accuracy:', accuracy)\n",
    "    precision=compute_precision(tp_r,fp_r)\n",
    "    print('precision:', precision)\n",
    "    recall=compute_recall(tp_r,fn_r)\n",
    "    print('recall:', recall)\n",
    "    f1_score=compute_f1_score(actual_y,predicted_y)\n",
    "    print('f1_score:', f1_score)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "#Function to perform training with Entropy\n",
    "dt=DecisionTreeClassifier(criterion=\"entropy\", random_state=0, max_depth=2)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130  12]\n",
      " [ 21  50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.92      0.89       142\n",
      "         2.0       0.81      0.70      0.75        71\n",
      "\n",
      "    accuracy                           0.85       213\n",
      "   macro avg       0.83      0.81      0.82       213\n",
      "weighted avg       0.84      0.85      0.84       213\n",
      "\n",
      "TP for dt is: 50\n",
      "TN for dt is: 130\n",
      "FP for dt is: 21\n",
      "FN for dt is: 12\n",
      "accuracy: 61.267605633802816\n",
      "precision: 70.4225352112676\n",
      "recall: 80.64516129032258\n",
      "f1_score: 0.7518796992481203\n"
     ]
    }
   ],
   "source": [
    "# Function to make prediction\n",
    "y_pred_en=dt.predict(X_test)\n",
    "#checking performance of the model\n",
    "print(confusion_matrix(y_test, y_pred_en))\n",
    "print(classification_report(y_test, y_pred_en))\n",
    "performance_of_the_model(y_test, y_pred_en)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   3]\n",
      " [ 23  48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.98      0.91       142\n",
      "         2.0       0.94      0.68      0.79        71\n",
      "\n",
      "    accuracy                           0.88       213\n",
      "   macro avg       0.90      0.83      0.85       213\n",
      "weighted avg       0.89      0.88      0.87       213\n",
      "\n",
      "TP for dt is: 48\n",
      "TN for dt is: 139\n",
      "FP for dt is: 23\n",
      "FN for dt is: 3\n",
      "accuracy: 65.48356807511738\n",
      "precision: 67.6056338028169\n",
      "recall: 94.11764705882354\n",
      "f1_score: 0.7868852459016393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SARA\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "rfc=RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "predictionRF=rfc.predict(X_test)\n",
    "#checking performance of the model\n",
    "print(confusion_matrix(y_test, predictionRF))\n",
    "print(classification_report(y_test, predictionRF))\n",
    "performance_of_the_model(y_test, predictionRF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.594519519326424"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define K\n",
    "import math\n",
    "math.sqrt(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137   5]\n",
      " [ 19  52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.88      0.96      0.92       142\n",
      "         2.0       0.91      0.73      0.81        71\n",
      "\n",
      "    accuracy                           0.89       213\n",
      "   macro avg       0.90      0.85      0.87       213\n",
      "weighted avg       0.89      0.89      0.88       213\n",
      "\n",
      "TP for dt is: 52\n",
      "TN for dt is: 137\n",
      "FP for dt is: 19\n",
      "FN for dt is: 5\n",
      "accuracy: 64.56338028169014\n",
      "precision: 73.2394366197183\n",
      "recall: 91.2280701754386\n",
      "f1_score: 0.8124999999999999\n"
     ]
    }
   ],
   "source": [
    "#K-NN classifier\n",
    "knn=KNeighborsClassifier(n_neighbors=13, p=2, metric='euclidean')\n",
    "\n",
    "#predict the test set results\n",
    "knn.fit(X_train, y_train)\n",
    "predictionKNN=knn.predict(X_test)\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionKNN))\n",
    "print(classification_report(y_test, predictionKNN))\n",
    "performance_of_the_model(y_test, predictionKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   7]\n",
      " [ 17  54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.89      0.95      0.92       142\n",
      "         2.0       0.89      0.76      0.82        71\n",
      "\n",
      "    accuracy                           0.89       213\n",
      "   macro avg       0.89      0.86      0.87       213\n",
      "weighted avg       0.89      0.89      0.88       213\n",
      "\n",
      "TP for dt is: 54\n",
      "TN for dt is: 135\n",
      "FP for dt is: 17\n",
      "FN for dt is: 7\n",
      "accuracy: 63.63380281690141\n",
      "precision: 76.05633802816901\n",
      "recall: 88.52459016393442\n",
      "f1_score: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# svm Classifier\n",
    "clf=svm.SVC(kernel=\"linear\", C=0.8,probability=True)\n",
    "clf.fit(X_train,y_train)\n",
    "predictionSVM=clf.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionSVM))\n",
    "print(classification_report(y_test, predictionSVM))\n",
    "performance_of_the_model(y_test, predictionSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136   6]\n",
      " [ 18  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.88      0.96      0.92       142\n",
      "         2.0       0.90      0.75      0.82        71\n",
      "\n",
      "    accuracy                           0.89       213\n",
      "   macro avg       0.89      0.85      0.87       213\n",
      "weighted avg       0.89      0.89      0.88       213\n",
      "\n",
      "TP for dt is: 53\n",
      "TN for dt is: 136\n",
      "FP for dt is: 18\n",
      "FN for dt is: 6\n",
      "accuracy: 64.09859154929578\n",
      "precision: 74.64788732394366\n",
      "recall: 89.83050847457628\n",
      "f1_score: 0.8153846153846154\n"
     ]
    }
   ],
   "source": [
    "#Bagging Classifier\n",
    "bg=BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1.0, n_estimators=10)\n",
    "bg=BaggingClassifier()\n",
    "bg.fit(X_train,y_train)\n",
    "predictionbg=bg.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionbg))\n",
    "print(classification_report(y_test, predictionbg))\n",
    "performance_of_the_model(y_test, predictionbg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120  22]\n",
      " [ 13  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.85      0.87       142\n",
      "         2.0       0.72      0.82      0.77        71\n",
      "\n",
      "    accuracy                           0.84       213\n",
      "   macro avg       0.81      0.83      0.82       213\n",
      "weighted avg       0.84      0.84      0.84       213\n",
      "\n",
      "TP for dt is: 58\n",
      "TN for dt is: 120\n",
      "FP for dt is: 13\n",
      "FN for dt is: 22\n",
      "accuracy: 56.610328638497656\n",
      "precision: 81.69014084507042\n",
      "recall: 72.5\n",
      "f1_score: 0.7682119205298013\n"
     ]
    }
   ],
   "source": [
    "#Boosting Classifier- Ada Boost\n",
    "adb=AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=1)\n",
    "adb.fit(X_train,y_train)\n",
    "predictionadb=adb.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionadb))\n",
    "print(classification_report(y_test, predictionadb))\n",
    "performance_of_the_model(y_test, predictionadb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127  15]\n",
      " [ 16  55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.89      0.89      0.89       142\n",
      "         2.0       0.79      0.77      0.78        71\n",
      "\n",
      "    accuracy                           0.85       213\n",
      "   macro avg       0.84      0.83      0.84       213\n",
      "weighted avg       0.85      0.85      0.85       213\n",
      "\n",
      "TP for dt is: 55\n",
      "TN for dt is: 127\n",
      "FP for dt is: 16\n",
      "FN for dt is: 15\n",
      "accuracy: 59.882629107981224\n",
      "precision: 77.46478873239437\n",
      "recall: 78.57142857142857\n",
      "f1_score: 0.7801418439716311\n"
     ]
    }
   ],
   "source": [
    "#Naive bayse Classifier\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "predictionnb=nb.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionnb))\n",
    "print(classification_report(y_test, predictionnb))\n",
    "performance_of_the_model(y_test, predictionnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[118  24]\n",
      " [ 14  57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.89      0.83      0.86       142\n",
      "         2.0       0.70      0.80      0.75        71\n",
      "\n",
      "    accuracy                           0.82       213\n",
      "   macro avg       0.80      0.82      0.81       213\n",
      "weighted avg       0.83      0.82      0.82       213\n",
      "\n",
      "TP for dt is: 57\n",
      "TN for dt is: 118\n",
      "FP for dt is: 14\n",
      "FN for dt is: 24\n",
      "accuracy: 55.666666666666664\n",
      "precision: 80.28169014084507\n",
      "recall: 70.37037037037037\n",
      "f1_score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#creat Multilayer neural network Classifier\n",
    "nn=MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "nn.fit(X_train,y_train)\n",
    "predictionNN=nn.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionNN))\n",
    "print(classification_report(y_test, predictionNN))\n",
    "performance_of_the_model(y_test, predictionNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136   6]\n",
      " [ 18  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.88      0.96      0.92       142\n",
      "         2.0       0.90      0.75      0.82        71\n",
      "\n",
      "    accuracy                           0.89       213\n",
      "   macro avg       0.89      0.85      0.87       213\n",
      "weighted avg       0.89      0.89      0.88       213\n",
      "\n",
      "TP for dt is: 53\n",
      "TN for dt is: 136\n",
      "FP for dt is: 18\n",
      "FN for dt is: 6\n",
      "accuracy: 64.09859154929578\n",
      "precision: 74.64788732394366\n",
      "recall: 89.83050847457628\n",
      "f1_score: 0.8153846153846154\n"
     ]
    }
   ],
   "source": [
    "#Voting Classifier- Multiple Model Ensemble\n",
    "evc=VotingClassifier(estimators=[('dt',dt),('rfc',rfc),('clf',clf),('knn',knn),('bg',bg),('adb',adb),('nb',nb),('nn',nn)], voting='soft')\n",
    "evc.fit(X_train,y_train)\n",
    "predictionevc=evc.predict(X_test)\n",
    "\n",
    "#Evaluate model\n",
    "print(confusion_matrix(y_test, predictionevc))\n",
    "print(classification_report(y_test, predictionevc))\n",
    "performance_of_the_model(y_test, predictionevc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining learning classifiers\n",
    "names=[\"Decision_Tree\", \"Random_Forest\", \"linear_SVM\",\"Nearest_Neighbors\",\"Bagging\",\"AdaBoost\",\"Naive_Bayes\",\"Neural_Net\",\"Vothing_classifier\"]\n",
    "classifiers = [dt,rfc,clf,knn,bg,adb,nb,nn,evc]\n",
    "\n",
    "#Build Model, Apply Model on Test Data & Record Accuracy Scores\n",
    "scores = []\n",
    "Fmeasure=[]\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction=clf.predict(X_test)\n",
    "    scores.append(compute_tp_tn_fn_fp(y_test, prediction))\n",
    "    f1_score=compute_f1_score(y_test,prediction)\n",
    "    Fmeasure.append(f1_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 130, 21, 12),\n",
       " (48, 139, 23, 3),\n",
       " (54, 135, 17, 7),\n",
       " (52, 137, 19, 5),\n",
       " (53, 134, 18, 8),\n",
       " (57, 117, 14, 25),\n",
       " (55, 127, 16, 15),\n",
       " (53, 125, 18, 17),\n",
       " (53, 135, 18, 7)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=[i[0] for i in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 48, 54, 52, 53, 57, 55, 53, 53]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=[i[2] for i in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 23, 17, 19, 18, 14, 16, 18, 18]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7518796992481203,\n",
       " 0.7868852459016393,\n",
       " 0.8181818181818182,\n",
       " 0.8124999999999999,\n",
       " 0.803030303030303,\n",
       " 0.7450980392156864,\n",
       " 0.7801418439716311,\n",
       " 0.75177304964539,\n",
       " 0.8091603053435115]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random (chance) Prediction: AUROC = 0.500\n",
      "Decision_Tree: AUROC = 0.829\n",
      "Random_Forest: AUROC = 0.895\n",
      "linear_SVM: AUROC = 0.896\n",
      "Nearest_Neighbors: AUROC = 0.907\n",
      "Bagging: AUROC = 0.908\n",
      "AdaBoost: AUROC = 0.813\n",
      "Naive Bayes: AUROC = 0.854\n",
      "Neural_Net: AUROC = 0.824\n",
      "Vothing_classifier: AUROC = 0.896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define Prediction probabilities\n",
    "r_probs_ = [0 for _ in range(len(y_test))]\n",
    "dt_probs = dt.predict_proba(X_test)\n",
    "rfc_probs = rfc.predict_proba(X_test)\n",
    "clf_probs = clf.predict_proba(X_test)\n",
    "knn_probs = knn.predict_proba(X_test)\n",
    "bg_probs = bg.predict_proba(X_test)\n",
    "adb_probs = adb.predict_proba(X_test)\n",
    "nb_probs = nb.predict_proba(X_test)\n",
    "nn_probs = nn.predict_proba(X_test)\n",
    "evc_probs = evc.predict_proba(X_test)\n",
    "\n",
    "#Probabilities for the positive outcome is kept\n",
    "dt_probs_ = dt_probs[:,1]\n",
    "rfc_probs_ = rfc_probs[:,1]\n",
    "clf_probs_ = clf_probs[:,1]\n",
    "knn_probs_ = knn_probs[:,1]\n",
    "bg_probs_ = bg_probs[:,1]\n",
    "adb_probs_ = adb_probs[:,1]\n",
    "nb_probs_ = nb_probs[:,1]\n",
    "nn_probs_ = nn_probs[:,1]\n",
    "evc_probs_ = evc_probs[:,1]\n",
    "#ROC is the receiver operating characteristic AUROC is the area under the ROC curve\n",
    "result=[]\n",
    "r_auc = roc_auc_score(y_test, r_probs_)\n",
    "dt_auc = roc_auc_score(y_test, dt_probs_)\n",
    "result.append(dt_auc)\n",
    "rfc_auc = roc_auc_score(y_test, rfc_probs_)\n",
    "result.append(rfc_auc)\n",
    "clf_auc = roc_auc_score(y_test, clf_probs_)\n",
    "result.append(clf_auc)\n",
    "knn_auc = roc_auc_score(y_test, knn_probs_)\n",
    "result.append(knn_auc)\n",
    "bg_auc = roc_auc_score(y_test, bg_probs_)\n",
    "result.append(bg_auc)\n",
    "adb_auc = roc_auc_score(y_test, adb_probs_)\n",
    "result.append(adb_auc)\n",
    "nb_auc = roc_auc_score(y_test, nb_probs_)\n",
    "result.append(nb_auc)\n",
    "nn_auc = roc_auc_score(y_test, nn_probs_)\n",
    "result.append(nn_auc)\n",
    "evc_auc = roc_auc_score(y_test, evc_probs_)\n",
    "result.append(evc_auc)\n",
    "\n",
    "#Print AUROC scores\n",
    "print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))\n",
    "print('Decision_Tree: AUROC = %.3f' % (dt_auc))\n",
    "print('Random_Forest: AUROC = %.3f' % (rfc_auc))\n",
    "print('linear_SVM: AUROC = %.3f' % (clf_auc))\n",
    "print('Nearest_Neighbors: AUROC = %.3f' % (knn_auc))\n",
    "print('Bagging: AUROC = %.3f' % (bg_auc))\n",
    "print('AdaBoost: AUROC = %.3f' % (adb_auc))\n",
    "print('Naive Bayes: AUROC = %.3f' % (nb_auc))\n",
    "print('Neural_Net: AUROC = %.3f' % (nn_auc))\n",
    "print('Vothing_classifier: AUROC = %.3f' % (evc_auc))\n",
    "\n",
    "#Calculate ROC curve\n",
    "r_fpr, r_tpr, threshold = roc_curve(y_test, r_probs_,pos_label=1,drop_intermediate=True)\n",
    "dt_fpr, dt_tpr, threshold = roc_curve(y_test, dt_probs_,pos_label=1,drop_intermediate=True)\n",
    "rfc_fpr, rfc_tpr, threshold = roc_curve(y_test, rfc_probs_,pos_label=1,drop_intermediate=True)\n",
    "clf_fpr, clf_tpr, threshold= roc_curve(y_test, clf_probs_,pos_label=1,drop_intermediate=True)\n",
    "knn_fpr, knn_tpr,threshold = roc_curve(y_test, knn_probs_,pos_label=1,drop_intermediate=True)\n",
    "bg_fpr, bg_tpr, threshold= roc_curve(y_test, bg_probs_,pos_label=1,drop_intermediate=True)\n",
    "adb_fpr, adb_tpr, threshold = roc_curve(y_test, adb_probs_,pos_label=1,drop_intermediate=True)\n",
    "nb_fpr, nb_tpr, threshold = roc_curve(y_test, nb_probs_,pos_label=1,drop_intermediate=True)\n",
    "nn_fpr, nb_tpr, threshold = roc_curve(y_test, nn_probs_,pos_label=1,drop_intermediate=True)\n",
    "evc_fpr, evc_tpr, threshold= roc_curve(y_test, evc_probs_,pos_label=1,drop_intermediate=True)\n",
    "\n",
    "\n",
    "# #Plot the ROC curve\n",
    "# plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n",
    "# plt.plot(dt_fpr, dt_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % dt_auc)\n",
    "# plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Naive Bayes (AUROC = %0.3f)' % rfc_auc)\n",
    "# plt.plot(clf_fpr, clf_tpr, marker='.', label='linear_SVM (AUROC = %0.3f)' % clf_auc)\n",
    "# plt.plot(knn_fpr, knn_tpr, marker='.', label='Nearest_Neighbors(AUROC = %0.3f)' % knn_auc)\n",
    "# plt.plot(bg_fpr, bg_tpr, marker='.', label='Bagging (AUROC = %0.3f)' % bg_auc)\n",
    "# plt.plot(adb_fpr, adb_tpr, marker='.', label='AdaBoost (AUROC = %0.3f)' % adb_auc)\n",
    "# plt.plot(nb_fpr, nb_tpr, marker='.', label='Naive_Bayes (AUROC = %0.3f)' % nb_auc)\n",
    "# plt.plot(nn_fpr, nn_tpr, marker='.', label='Neural_Net (AUROC = %0.3f)' % nn_auc)\n",
    "# plt.plot(evc_fpr, evc_tpr, marker='.', label='Vothing_classifier (AUROC = %0.3f)' % evc_auc)\n",
    "# # Title\n",
    "# plt.title('ROC Plot')\n",
    "# plt.legend(loc = 'lower right')\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8288038087681016,\n",
       " 0.8951596905375917,\n",
       " 0.8964491172386432,\n",
       " 0.9068141241817099,\n",
       " 0.9080539575481055,\n",
       " 0.8133802816901409,\n",
       " 0.8537492560999802,\n",
       " 0.824489188653045,\n",
       " 0.8964491172386432]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame of model performance\n",
    "df = pd.DataFrame()\n",
    "df['Machine Learning algorithms'] = names\n",
    "df['F-measure-Twitter'] = Fmeasure \n",
    "df['TP-Twitter']=tp\n",
    "df['FP-Twitter']=fp\n",
    "df['AUC-Twitter']=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col1 {\n",
       "            background-color:  #f3e4e8;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col2 {\n",
       "            background-color:  #eed7dd;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col3 {\n",
       "            background-color:  #d69eac;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col4 {\n",
       "            background-color:  #f0dde2;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col1 {\n",
       "            background-color:  #dfb3be;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col2 {\n",
       "            background-color:  #f7edf0;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col3 {\n",
       "            background-color:  #cc8899;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col4 {\n",
       "            background-color:  #d295a5;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col1 {\n",
       "            background-color:  #cc8899;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col2 {\n",
       "            background-color:  #daaab6;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col3 {\n",
       "            background-color:  #e9cbd3;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col4 {\n",
       "            background-color:  #d194a4;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col1 {\n",
       "            background-color:  #cf909f;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col2 {\n",
       "            background-color:  #e4c0c9;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col3 {\n",
       "            background-color:  #dfb5bf;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col4 {\n",
       "            background-color:  #cd899a;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col1 {\n",
       "            background-color:  #d59dab;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col2 {\n",
       "            background-color:  #dfb5bf;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col3 {\n",
       "            background-color:  #e4c0c9;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col4 {\n",
       "            background-color:  #cc8899;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col1 {\n",
       "            background-color:  #f7edf0;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col2 {\n",
       "            background-color:  #cc8899;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col3 {\n",
       "            background-color:  #f7edf0;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col4 {\n",
       "            background-color:  #f7edf0;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col1 {\n",
       "            background-color:  #e3bdc6;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col2 {\n",
       "            background-color:  #d69eac;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col3 {\n",
       "            background-color:  #eed7dd;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col4 {\n",
       "            background-color:  #e5c2cb;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col1 {\n",
       "            background-color:  #f3e4e8;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col2 {\n",
       "            background-color:  #dfb5bf;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col3 {\n",
       "            background-color:  #e4c0c9;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col4 {\n",
       "            background-color:  #f2e1e5;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col1 {\n",
       "            background-color:  #d194a4;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col2 {\n",
       "            background-color:  #dfb5bf;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col3 {\n",
       "            background-color:  #e4c0c9;\n",
       "            color:  #000000;\n",
       "        }    #T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col4 {\n",
       "            background-color:  #d194a4;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Machine Learning algorithms</th>        <th class=\"col_heading level0 col1\" >F-measure-Twitter</th>        <th class=\"col_heading level0 col2\" >TP-Twitter</th>        <th class=\"col_heading level0 col3\" >FP-Twitter</th>        <th class=\"col_heading level0 col4\" >AUC-Twitter</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col0\" class=\"data row0 col0\" >Decision_Tree</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col1\" class=\"data row0 col1\" >0.75188</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col2\" class=\"data row0 col2\" >50</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col3\" class=\"data row0 col3\" >21</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row0_col4\" class=\"data row0 col4\" >0.828804</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col0\" class=\"data row1 col0\" >Random_Forest</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col1\" class=\"data row1 col1\" >0.786885</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col2\" class=\"data row1 col2\" >48</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col3\" class=\"data row1 col3\" >23</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row1_col4\" class=\"data row1 col4\" >0.89516</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col0\" class=\"data row2 col0\" >linear_SVM</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col1\" class=\"data row2 col1\" >0.818182</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col2\" class=\"data row2 col2\" >54</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col3\" class=\"data row2 col3\" >17</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row2_col4\" class=\"data row2 col4\" >0.896449</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col0\" class=\"data row3 col0\" >Nearest_Neighbors</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col1\" class=\"data row3 col1\" >0.8125</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col2\" class=\"data row3 col2\" >52</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col3\" class=\"data row3 col3\" >19</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row3_col4\" class=\"data row3 col4\" >0.906814</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col0\" class=\"data row4 col0\" >Bagging</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col1\" class=\"data row4 col1\" >0.80303</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col2\" class=\"data row4 col2\" >53</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col3\" class=\"data row4 col3\" >18</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row4_col4\" class=\"data row4 col4\" >0.908054</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col0\" class=\"data row5 col0\" >AdaBoost</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col1\" class=\"data row5 col1\" >0.745098</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col2\" class=\"data row5 col2\" >57</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col3\" class=\"data row5 col3\" >14</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row5_col4\" class=\"data row5 col4\" >0.81338</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col0\" class=\"data row6 col0\" >Naive_Bayes</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col1\" class=\"data row6 col1\" >0.780142</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col2\" class=\"data row6 col2\" >55</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col3\" class=\"data row6 col3\" >16</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row6_col4\" class=\"data row6 col4\" >0.853749</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col0\" class=\"data row7 col0\" >Neural_Net</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col1\" class=\"data row7 col1\" >0.751773</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col2\" class=\"data row7 col2\" >53</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col3\" class=\"data row7 col3\" >18</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row7_col4\" class=\"data row7 col4\" >0.824489</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col0\" class=\"data row8 col0\" >Vothing_classifier</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col1\" class=\"data row8 col1\" >0.80916</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col2\" class=\"data row8 col2\" >53</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col3\" class=\"data row8 col3\" >18</td>\n",
       "                        <td id=\"T_121539d0_1e0e_11eb_9d31_d053490e15f2row8_col4\" class=\"data row8 col4\" >0.896449</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20843d28d08>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding colors to the data frame\n",
    "cm = sns.light_palette(\"#CC8899\", as_cmap=True)\n",
    "s = df.style.background_gradient(cmap=cm)\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
